# HATVP RAG Experiment  

The idea is to create an assistant that can answer questions about declarations of interest published by the HATVP.  
To mitigate hallucinations, as it is a sensitive matter, we use the RAG technique.  
This will ensure that our responses are grounded in truth.  

We use the open data declarations dataset.  
[Link to the open data page](https://www.hatvp.fr/open-data/)  
[Link to the XML dataset used throughout this project](https://www.hatvp.fr/livraison/merge/declarations.xml)  

LLMs used in this project are mostly Llama8B and Llama70B, but some tests were also made using GPT4.  
We use groq cloud for free API inference.  
OpenAI was used mainly for embedding.  

The datasets are availiable on Huggingface Hub, links in the relevant notebooks below.  

## What is RAG ?


Retrieval Augmented Generation (RAG) is a technique that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. In RAG, a retrieval component first searches for relevant documents or pieces of information from a large dataset based on the input query. Then, a generative model, such as a language model, uses this retrieved information to produce a more accurate and contextually relevant response. This approach leverages the strengths of both retrieval and generation, resulting in more informative and coherent outputs.

## How do we do it in this project ?

First, we split the single XML file into single declaration XML files.  
We then convert them to: 
* JSON, to feed entirely in the LLM context while using 30% less characters in context  
* Markdown/text values for indexing  
* We also extracted a few fields like name, surname, and last job title, also for indexing purposes  

We then index either the text values, or only an index (made of relevant text fields) using embeddings generated by openai text embedding LARGE model.  
Observations show that French sensitivity is increased when using the large model instead of small, so the increased cost is worth it.  
Eg. Making the difference between 'Maire' and 'Adjoint au Maire'.  

Once our XML documents are converted to JSON, and indexed by an embedding of a text index, we are ready to query an LLM.  

When the user writes a questions, we run the query through openai large embedding model.  
We then retrieve the closest document and add the JSON declaration inside the LLM context.  
Finally, the LLM answers the query using this enriched context.  

Eg. What was the salary of Damien Abad in 2019?  
1. Retrieve Damien Abad declaration  
2. Augment context with JSON declaration of Damien Abad  
3. Query Llama70B via groq with this augmented context and the user query ('What was the salary of Damien Abad in 2019?')  
4. Display the answer.  

## Notebooks: what do they do?  

These notebooks helped create the datasets needed for the experiments:  
* `Part_1_HATVP_RAG_Convert_XML_to_JSON`
* `Part_2_HATVP_RAG_Convert_JSON_to_text`
* `Part_3_HATVP_RAG_Convert_JSON_to_text_index`
* `Part_4_HATVP_RAG_Convert_JSON_to_text_with_metadata`

This notebook contains many example of working document retrieval for context enrichment, and LLM inference (complete RAG system):  
* `Part_5_HATVP_Llangchain_on_JSON_dataset_3_NO_chunks`

There are several experiments in document retrieval, like:  
* "give me the salary of person X on year Y" 
* "give me the salary of a person whose spouse is a nurse"  
* "give me the salary of a person that is a Mayor"  
* etc.
We also provide the LLM response on the retrieved contexts and show that the LLM doesn't hallucintate.  


This notebook contains the minimal code needed to perform RAG on the HATVP dataset:  
* `Part_6_HATVP_RAG_Light_functional_version`
Please do play with it, ask it questions and see if it can retrieve the documents and answer truthfully.  